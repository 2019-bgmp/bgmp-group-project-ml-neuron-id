#!/bin/bash
#SBATCH --account=bgmp          ### SLURM account which will be charged for the job
#SBATCH --partition=gpu        ### Partition (like a queue in PBS)
#SBATCH --job-name=training      ### Job Name
#SBATCH --output=./slurm_output/train_spot.out         ### File in which to store job output
#SBATCH --error=./slurm_output/train_spot.err          ### File in which to store job error messages
#SBATCH --time=0-00:10:00       ### Wall clock time limit in Days-HH:MM:SS
#SBATCH --gres=gpu:1
#SBATCH --nodes=1               ### Node count required for the job (usually 1)
#SBATCH --ntasks-per-node=1     ### Nuber of tasks to be launched per Node (usually 1)
#SBATCH --cpus-per-task=1       ### Number of cpus (cores) per task
#SBATCH --mail-user=jaredgalloway07@gmail.com
#SBATCH --mail-type=ALL

# Load modules
# conda activate bgmp_py3
ml python3
ml cuda/9.0

# SCRIPT 
/usr/bin/time -v python3 train_spot.py
